# -*- coding: utf-8 -*-
"""feature_importances.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZcggANDCN63JBlmeJzwWW55rfRfIXaa1
"""

import pandas as pd

df=pd.read_csv("all_features2.csv" )

df['datetime'] = pd.to_datetime(df['datetime'] , utc=True)
#df.set_index('datetime', inplace=True)
df.head()

df.set_index('datetime', inplace=True)

!pip install shap

!pip install category_encoders

import statsmodels.api as sm
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
import shap
import category_encoders as ce

import xgboost as xgb

df.isnull().sum()

target = 'Load'
y = df[target]
X = df.drop(columns=[target,
                    # 'Load_previous_hour'
                     #	'Load_same_hour_previous_day',	'Load_same_hour_previous_week',	'year',	'month',	'day',
                     #'day_of_week'	,'day_of_year'	,'date_issued:hour','day_part_encoded'	,'is_year_start',	'is_quarter_start'	,'is_month_start'	,
                     #'is_month_end',	'is_weekend',	'week_number'
                     ])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2 #random_state=1066
                                                    )

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train, y_train)
X_test_loo = encoder.transform(X_test)

model = xgb.XGBRegressor(n_estimators=500, max_depth=5, eta=0.05)
model.fit(X_train_loo, y_train)

rmse = np.sqrt(mean_squared_error(y_test, model.predict(np.ascontiguousarray(X_test_loo))))
rmse

perm_importance = permutation_importance(model, np.ascontiguousarray(X_test_loo), y_test, n_repeats=10, random_state=1066)
sorted_idx = perm_importance.importances_mean.argsort()

for i in sorted_idx:
    print(f"{X_test.columns[i]}: {perm_importance.importances_mean[i]}")



'''
fig = plt.figure(figsize=(12, 6))
plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])
plt.title('Permutation Importance')
'''

fig = plt.figure(figsize=(12, 6))
plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])
plt.title('Permutation Importance')

import pandas as pd

perm_importance_values = perm_importance.importances_mean
sorted_idx = perm_importance_values.argsort()

# Extract the feature names and importance values using indexing
feature_names = X_test.columns[sorted_idx]
sorted_importance_values = perm_importance_values[sorted_idx]

# Create a DataFrame to store the feature names and importance values
importance_data = pd.DataFrame({'Feature Name': feature_names, 'Importance_Perm': sorted_importance_values})

# Print the importance data
print(importance_data)

explainer = shap.Explainer(model)
shap_values = explainer(np.ascontiguousarray(X_test_loo))
shap_importance = shap_values.abs.mean(0).values
sorted_idx = shap_importance.argsort()



import pandas as pd


# Extract the feature names and importance values using indexing
feature_names = X_test.columns[sorted_idx]
sorted_importance_values = shap_importance[sorted_idx]

# Create a DataFrame to store the feature names and importance values
importance_data_SHAP = pd.DataFrame({'Feature Name': feature_names, 'Importance_SHAP': sorted_importance_values})

# Print the importance data
print(importance_data_SHAP)

fig = plt.figure(figsize=(12, 6))
plt.barh(range(len(sorted_idx)), shap_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])
plt.title('SHAP Importance')

shap.plots.bar(shap_values, max_display=X_test_loo.shape[0])

feature_names = X_test.columns
top_features = feature_names[:11]  # Extract the feature names for indices 0 to 10
print(top_features)

import pandas as pd
from xgboost import XGBRegressor
import matplotlib.pyplot as plt


X = df.drop('Load', axis=1)
y = df['Load']


model = XGBRegressor()


model.fit(X, y)

model = XGBRegressor(importance_type='gain')
model.fit(X, y)

importances_XGB = model.feature_importances_
#pd.Series(importances, index=X.columns).sort_values()




importance_data_XGB = pd.Series(importances_XGB, index=X.columns).sort_values().reset_index()
importance_data_XGB.columns = ['Feature Name', 'Importance']

print(importance_data_XGB)

"""merge all"""

merged_data = pd.merge(importance_data_XGB, importance_data_SHAP, on='Feature Name', how='outer')
merged_data2 = pd.merge(merged_data, importance_data, on='Feature Name', how='outer')

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

# Select the importance measures of interest
importance_measures = ['Importance', 'Importance_SHAP', 'Importance_Perm']

# Normalize the importance values
scaler = MinMaxScaler()
normalized_importance = scaler.fit_transform(merged_data2[importance_measures])

# Create a composite score by averaging the normalized importance values
composite_score = np.mean(normalized_importance, axis=1)

# Apply K-means clustering to group the features
num_clusters = 5  # Set the desired number of clusters (increased to 5)
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
feature_clusters = kmeans.fit_predict(composite_score.reshape(-1, 1))

# Add the cluster labels to the merged_data2 DataFrame
merged_data2['Cluster'] = feature_clusters

# Print the features in each cluster
for cluster_id in range(num_clusters):
    cluster_features = merged_data2[merged_data2['Cluster'] == cluster_id]['Feature Name'].tolist()
    print(f"Cluster {cluster_id + 1}: {cluster_features}")